import random
from dataclasses import dataclass
from typing import Union, List

import datasets
from torch.utils.data import Dataset
from transformers import PreTrainedTokenizer, BatchEncoding, DataCollatorWithPadding

from tqdm import tqdm
import json
from .arguments import DataArguments
from .trainer import DenseTrainer

import logging
logger = logging.getLogger(__name__)

import collections


MaskedLmInstance = collections.namedtuple("MaskedLmInstance",
                                          ["index", "label"])


def create_masked_lm_predictions(tokens, masked_lm_prob,
                                 max_predictions_per_seq, vocab_words, rng):
  """Creates the predictions for the masked LM objective."""

  cand_indexes = []
  for (i, token) in enumerate(tokens):
    if token == "[CLS]" or token == "[SEP]":
      continue
    # Whole Word Masking means that if we mask all of the wordpieces
    # corresponding to an original word. When a word has been split into
    # WordPieces, the first token does not have any marker and any subsequence
    # tokens are prefixed with ##. So whenever we see the ## token, we
    # append it to the previous set of word indexes.
    #
    # Note that Whole Word Masking does *not* change the training code
    # at all -- we still predict each WordPiece independently, softmaxed
    # over the entire vocabulary.
    do_whole_word_mask = 0
    if (do_whole_word_mask and len(cand_indexes) >= 1 and
        token.startswith("##")):
      cand_indexes[-1].append(i)
    else:
      cand_indexes.append([i])

  rng.shuffle(cand_indexes)

  output_tokens = list(tokens)

  num_to_predict = min(max_predictions_per_seq,
                       max(1, int(round(len(tokens) * masked_lm_prob))))

  masked_lms = []
  covered_indexes = set()
  for index_set in cand_indexes:
    if len(masked_lms) >= num_to_predict:
      break
    # If adding a whole-word mask would exceed the maximum number of
    # predictions, then just skip this candidate.
    if len(masked_lms) + len(index_set) > num_to_predict:
      continue
    is_any_index_covered = False
    for index in index_set:
      if index in covered_indexes:
        is_any_index_covered = True
        break
    if is_any_index_covered:
      continue
    for index in index_set:
      covered_indexes.add(index)

      masked_token = None
      # 80% of the time, replace with [MASK]
      if rng.random() < 0.8:
        masked_token = "[MASK]"
      else:
        # 10% of the time, keep original
        if rng.random() < 0.5:
          masked_token = tokens[index]
        # 10% of the time, replace with random word
        else:
          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]

      output_tokens[index] = masked_token

      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))
  assert len(masked_lms) <= num_to_predict
  masked_lms = sorted(masked_lms, key=lambda x: x.index)

  masked_lm_positions = []
  masked_lm_labels = []
  for p in masked_lms:
    masked_lm_positions.append(p.index)
    masked_lm_labels.append(p.label)

  return (output_tokens, masked_lm_positions, masked_lm_labels)



class Caseformer_TrainDataset(Dataset):
    def __init__(
            self,
            data_args: DataArguments,
            path_to_data: Union[List[str], datasets.Dataset],
            tokenizer: PreTrainedTokenizer,
            trainer: DenseTrainer = None,
    ):
        if isinstance(path_to_data, datasets.Dataset):
            self.train_data = path_to_data
        else:
            print('------------------------------------')
            print(path_to_data)
            print('------------------------------------')
            self.train_data = datasets.load_dataset(
                '/home/swh/json.py',
                data_files=path_to_data,
                ignore_verifications=False,
                features=datasets.Features({
                    'qid':
                        datasets.Value('string')
                    ,
                    'pos': [
                        datasets.Value('string'),
                    ],
                    'neg': [
                        datasets.Value('string'),
                    ]}
                )
            )['train']
#           self.train_data = datasets.load_dataset(
#                 '/home/swh/json.py',
#                 data_files=path_to_data,
#                 ignore_verifications=False,
#             )['train']

        print('reading corpus......')

        self.id_embedding = {}
        self.ids = []
        self.error = 0

        with open('/home/swh/legal/project/pretrain_legal/data/encode_ultra/encoded_corpus.json') as f:
            # lines = f.read().split('\n')
            # for line in tqdm(lines):
            for i in tqdm(range(5064492)):
                line = f.readline()
                if not line: continue
                tmp = json.loads(line)
                id = tmp['id']
                tokens = tmp['tokens'][0:512]
                self.id_embedding[id] = tokens
                self.ids.append(id)

        self.tok = tokenizer
        self.trainer = trainer

        self.data_args = data_args
        self.total_len = len(self.train_data)

    def create_one_example(self, text_encoding: List[int], is_query=False):
        item = self.tok.encode_plus(
            text_encoding,
            truncation='longest_first',
            max_length=self.data_args.q_max_len if is_query else self.data_args.p_max_len,
            padding=False,
            return_attention_mask=False,
            return_token_type_ids=False,
        )
        return item

    def __len__(self):
        return self.total_len

    def __getitem__(self, item) -> [BatchEncoding, List[BatchEncoding]]:
        # try:
        group = self.train_data[item]
        # print(group)
        epoch = int(self.trainer.state.epoch)

        _hashed_seed = hash(item + self.trainer.args.seed)

        qid = group['qid']
        qry = self.id_embedding[qid]

        encoded_query = self.create_one_example(qry, is_query=True)

        encoded_passages = []

        pos_ids = group['pos']
        neg_ids = group['neg']

        easy_ids = random.sample(self.ids,k=7*len(neg_ids))



        neg_ids = neg_ids + easy_ids


        group_positives = []

        for pos_id in pos_ids:
            group_positives.append(self.id_embedding[pos_id])

        group_negatives = []
        for neg_id in neg_ids:
            group_negatives.append(self.id_embedding[neg_id])

        pos_psg = group_positives[(_hashed_seed + epoch) % len(group_positives)]
        encoded_passages.append(self.create_one_example(pos_psg))

        negative_size = self.data_args.train_n_passages - 1
        if len(group_negatives) < negative_size:
            negs = random.choices(group_negatives, k=negative_size)
        elif self.data_args.train_n_passages == 1:
            negs = []
        else:
            _offset = epoch * negative_size % len(group_negatives)
            negs = [x for x in group_negatives]
            random.Random(_hashed_seed).shuffle(negs)
            negs = negs * 2
            negs = negs[_offset: _offset + negative_size]

        for neg_psg in negs:
            encoded_passages.append(self.create_one_example(neg_psg))
        

        return encoded_query, encoded_passages





class TrainDataset(Dataset):
    def __init__(
            self,
            data_args: DataArguments,
            path_to_data: Union[List[str], datasets.Dataset],
            tokenizer: PreTrainedTokenizer,
            trainer: DenseTrainer = None,
    ):
        if isinstance(path_to_data, datasets.Dataset):
            self.train_data = path_to_data
        else:
            self.train_data = datasets.load_dataset(
                '/home/swh/json.py',
                data_files=path_to_data,
                ignore_verifications=False,
            )['train']

        self.tok = tokenizer
        self.trainer = trainer

        self.data_args = data_args
        self.total_len = len(self.train_data)

    def create_one_example(self, text_encoding: List[int], is_query=False):
        item = self.tok.encode_plus(
            text_encoding,
            truncation='only_first',
            max_length=self.data_args.q_max_len if is_query else self.data_args.p_max_len,
            padding=False,
            return_attention_mask=False,
            return_token_type_ids=False,
        )
        return item

    def __len__(self):
        return self.total_len

    def __getitem__(self, item) -> [BatchEncoding, List[BatchEncoding]]:
        group = self.train_data[item]
        epoch = int(self.trainer.state.epoch)

        _hashed_seed = hash(item + self.trainer.args.seed)

        # qry = group['qry']
        qry = group['query']
        encoded_query = self.create_one_example(qry, is_query=True)

        encoded_passages = []
        # group_positives = group['pos']
        # group_negatives = group['neg']
        group_positives = group['positives']
        group_negatives = group['negatives']

        pos_psg = group_positives[(_hashed_seed + epoch) % len(group_positives)]
        encoded_passages.append(self.create_one_example(pos_psg))

        negative_size = self.data_args.train_n_passages - 1
        if len(group_negatives) < negative_size:
            negs = random.choices(group_negatives, k=negative_size)
        elif self.data_args.train_n_passages == 1:
            negs = []
        else:
            _offset = epoch * negative_size % len(group_negatives)
            negs = [x for x in group_negatives]
            random.Random(_hashed_seed).shuffle(negs)
            negs = negs * 2
            negs = negs[_offset: _offset + negative_size]

        for neg_psg in negs:
            encoded_passages.append(self.create_one_example(neg_psg))

        return encoded_query, encoded_passages


class EncodeDataset(Dataset):
    input_keys = ['text_id', 'text']

    def __init__(self, path_to_json: Union[List[str], datasets.Dataset], tokenizer: PreTrainedTokenizer, max_len=510):
        if isinstance(path_to_json, datasets.Dataset):
            self.encode_data = path_to_json
        else:
            self.encode_data = datasets.load_dataset(
                '/home/swh/json.py',
                data_files=path_to_json,
            )['train']
        self.tok = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.encode_data)

    def __getitem__(self, item) -> [str, BatchEncoding]:
        text_id, text = (self.encode_data[item][f] for f in self.input_keys)
        encoded_text = self.tok.encode_plus(
            text,
            max_length=self.max_len,
            truncation='only_first',
            padding=False,
            return_token_type_ids=False,
        )
        return text_id, encoded_text

class InferenceDataset(Dataset):
    input_keys = ['qid','did','qry','doc']

    def __init__(self, path_to_json: Union[List[str], datasets.Dataset], tokenizer: PreTrainedTokenizer, max_len):
        if isinstance(path_to_json, datasets.Dataset):
            self.encode_data = path_to_json
        else:
            self.encode_data = datasets.load_dataset(
                '/home/swh/json.py',
                data_files=path_to_json,
            )['train']
        self.tok = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.encode_data)

    def __getitem__(self, item) -> [str, BatchEncoding]:
        qid, did, qry, doc = (self.encode_data[item][f] for f in self.input_keys)
        encoded_doc = self.tok.encode_plus(
            doc,
            max_length=self.max_len,
            truncation='only_first',
            padding=False,
            return_token_type_ids=False,
        )

        encoded_qry = self.tok.encode_plus(
            qry,
            max_length=self.max_len,
            truncation='only_first',
            padding=False,
            return_token_type_ids=False,
        )


        return qid, did, encoded_qry, encoded_doc




class CorpusDataset(Dataset):
    input_keys = ['did','doc']

    def __init__(self, path_to_json: Union[List[str], datasets.Dataset], tokenizer: PreTrainedTokenizer, max_len):
        if isinstance(path_to_json, datasets.Dataset):
            self.encode_data = path_to_json
        else:
            self.encode_data = datasets.load_dataset(
                '/home/swh/json.py',
                data_files=path_to_json,
            )['train']
        self.tok = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.encode_data)

    def __getitem__(self, item) -> [str, BatchEncoding]:
        did, doc = (self.encode_data[item][f] for f in self.input_keys)
        encoded_doc = self.tok.encode_plus(
            doc,
            max_length=self.max_len,
            truncation='only_first',
            padding=False,
            return_token_type_ids=False,
        )

        
        return did, encoded_doc




@dataclass
class QPCollator(DataCollatorWithPadding):
    """
    Wrapper that does conversion from List[Tuple[encode_qry, encode_psg]] to List[qry], List[psg]
    and pass batch separately to the actual collator.
    Abstract out data detail for the model.
    """
    max_q_len: int = 510
    max_p_len: int = 510

    def __call__(self, features):
        qq = [f[0] for f in features]
        dd = [f[1] for f in features]

        if isinstance(qq[0], list):
            qq = sum(qq, [])
        if isinstance(dd[0], list):
            dd = sum(dd, [])

        q_collated = self.tokenizer.pad(
            qq,
            padding='max_length',
            max_length=self.max_q_len,
            return_tensors="pt",
        )
        d_collated = self.tokenizer.pad(
            dd,
            padding='max_length',
            max_length=self.max_p_len,
            return_tensors="pt",
        )

        return q_collated, d_collated


@dataclass
class EncodeCollator(DataCollatorWithPadding):
    def __call__(self, features):
        text_ids = [x[0] for x in features]
        text_features = [x[1] for x in features]
        collated_features = super().__call__(text_features)
        return text_ids, collated_features

@dataclass
class InferenceCollator(DataCollatorWithPadding):
    def __call__(self, features):
        qids = [x[0] for x in features]
        dids = [x[1] for x in features]
        queries = [x[2] for x in features]
        docs = [x[3] for x in features]



        collated_queries = super().__call__(queries)
        collated_docs = super().__call__(docs)
        return qids,dids,collated_queries,collated_docs

